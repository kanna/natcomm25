\section{Methods}
\label{sec:methods}


Our methodology is based on the implementation of a complete data cycle
to enhance model predictions in a coastal ocean domain through robotic
adaptive sampling and data assimilation. The approach consists of three
fundamental steps, executed iteratively on a daily basis:

\begin{description}

\item[Model Forecast and Uncertainty Projection]: A numerical ocean
  model provides a daily one-step forecast $\hat{\theta}(k+1, x, y)$
  of a target oceanic variable $\theta$, along with an associated
  uncertainty field $\sigma_{\hat{\theta}}(k+1, x, y)$, where $k$
  represents the current day and $(x, y)$ denote the geographical
  coordinates.  These outputs are organized into discrete spatial
  maps, $M_{\hat{\theta}}(x, y)$ and
  $M_{\sigma_{\hat{\theta}}}(x, y)$, representing, respectively, the
  predicted state and its uncertainty over a predefined grid covering
  the study area. 

\item \textbf{Targeted Sample Planning}: Using the uncertainty map
  $M_{\sigma_{\hat{\theta}}}(x, y)$ as input, a targeted sampling
  algorithm determines the set of trajectories for a fleet of $N$ AUVs
  for the next operational cycle. The goal is to maximize the
  accumulated uncertainty sampled along the vehicle paths, while
  satisfying vehicle-specific constraints. The planned trajectories
  are transmitted to the vehicles for execution.

\item \textbf{Data Collection and Assimilation}: Throughout the
  operational period, each AUV collects measurements of the target
  variable $\theta$ along its assigned path. After the mission is
  completed, the collected measurements are assimilated into the
  numerical model using an appropriate data assimilation scheme. This
  updated model state serves as the new initial condition for the next
  forecasting cycle, closing the loop.

\end{description}

While previous efforts have concentrated on a form on embedded
automated decision-making on AUVs
\cite{mcgann08a,mcgann08b,ryan10,py10,Das-2010-637,das10,olaya12,rajan12}
to demonstrate adaptive sampling, here we AUV trajectories are defined
by human-in-the-loop decisions apriori, with adaptation focused on
deployment based on $M_{\sigma_{\hat{\theta}}}(x, y)$. No automated
decisions were made on the AUVs. Loop closure in this work, refers to
the \emph{sample-assimilate-predict-direct} process as shown in
Fig. \ref{fig:loop-closure}.

\subsection{Model Forecast, Uncertainty Projection and Data Assimilation}

Sampling strategies rely on timely information about the spatial
variability of ocean properties. However, conventional numerical ocean
models are computationally intensive, and their runtime makes them
impractical for use in near-real-time mission planning \kcomment{Need
  a citation here}. While similar assimilation cycles could, in
principle, be implemented directly within deterministic numerical
models, this would require either substantially higher computational
resources or larger temporal horizons.


As a more practical alternative, we employ geostatistical stochastic
sequential simulation as a computationally efficient surrogate,
producing short-term spatial predictions of ocean variables together
with estimates of spatial uncertainty \cite{deutsch1992}. This
approach captures the essential variability of local ocean dynamics,
based on calibrated deterministic dynamic ocean models, at a fraction
of the computational cost of numerical models, while remaining
flexible enough to rapidly assimilate new in situ observations
\cite{Duarte2025}.  However, with this type of surrogate models we are
not explicitly considering the full-physics of the ecosystem and the
quality of the predictions directly depends on the accuracy of the
calibrated model. If the calibrated model is unable to capture the
true nature of the system the geostatistical predictions will likely
be inaccurate.  Equally, if there is a new oceanographic event that is
not present in the calibrated model, the geostatistical approach will
not be captured.

The methodology relies on ensembles of geostatistical realizations,
each representing a state of an ocean variable field conditioned by
\emph{a priori} deterministic ocean models \cite{CMEMS2017} and direct
observations. By computing the pointwise standard deviation across the
ensemble, we obtain spatial uncertainty maps that highlight regions
where predictions are more variable and therefore potentially more
informative for sampling. These maps serve as the basis for
identifying areas where new measurements are expected to maximize the
reduction of forecast error.

Beyond uncertainty prediction, the proposed framework also allows
direct assimilation of AUV acquired data. Data collected on a given
day is used to update the ocean variable forecast for the following
day by combining prior predictions with new measurements. The
simulation grid, in this case, includes two temporal layers: the first
containing the AUV measurements at their spatial locations and the
second corresponding to the new prediction step. With AUV sampling
denser than the grid resolution, measurements within the same grid
cell are averaged as a form of arithmetic upscaling
\cite{Duarte2025}. To incorporate spatial and temporal variability, we
apply direct sequential simulation with local means
\cite{soares2001direct}\footnote{Defined by kriging estimates and
  variances and conditioned to existing direct measurements and a
  spatial covariance matrix.}, where each simulated value is drawn
from its conditional distribution, accounting for both prior simulated
values and local mean models. Doing so enables consistent updating of
the variable field while integrating both direct AUV observations and
\textit{priori} information. The ensemble of realizations thus
provides an updated ocean forecast and a quantitative measure of
uncertainty.\kcomment{I think this para is very important and should
  be clear. An image of the process would be very helpful}
 
The continuity of the variable field in space and time is
characterized by variogram models fitted to long-term calibrated ocean
model data from the E.U. Copernicus Marine Service (CMS)
\cite{CMEMS2017}. For each depth of the numerical ocean model,
geostatistical simulations are carried out independently, using a
moving temporal window of fourteen previous days as conditioning data
(i.e., experimental data) to predict the subsequent day. The size of
the temporal window was achieved by trial-and-error using a calibrated
numerical model for the same region and under the same season, and
comparing the geostatistical predictions against the response of the
numerical ocean model. This sliding-window strategy strikes a balance
between forecast skill and computational feasibility, and can be
adapted according to the complexity of the oceanographic
conditions. \kcomment{AN EXPLANATION OF THE METHOD THAT PROCEEDS BY
  DEPTHS MAY BE USEFUL. Is PROPAGRATION DONE ONLY IN THE HORIZONTAL
  DOMAINS?}

This provides both a forecast of ocean variables being simulated, as
well as a quantitative assessment of the prediction uncertainty.  By
updating the forecasts with new AUV measurements through sequential
assimilation, the method progressively refines the variable field
while maintaining consistency with prior model dynamics. The resulting
forecast and uncertainty maps then form the input for the target
sampling algorithm, guiding the allocation of AUV trajectories towards
regions of greatest expected information gain. More details about the
model, its development, and application during the \proj experiment
can be found in \cite{Duarte2025}. 

\kcomment{PLEASE READ THESE ITEMS; THIS WAS INSERTED WED AT 1PM
\begin{itemize}
    \item WE HAVE NOT DISCUSSED THE FACT THAT THE INPUT TO THE SAMPLING ALGORITHM IS JUST AM HORIZONTAL SLICE OF THE ERROR PREDICTION
    \item but this can be somewhat compensated in the vertical domain by the YOYO
    \item but we may need to explain how have we selected the depth to be used in the sampling algorithm
    \item discussion about the mixed layer depth as suggested by Ajit?
\end{itemize}
}

\subsection{Sampling Algorithm}

The adaptive sampling problem posed is the design of vehicle
trajectories that maximize information gain
\cite{eidsvik2015,fossum18} extracted from a model-derived uncertainty
map, while satisfying operational AUV constraints. Each day, the
models provide both a forecast field and its associated uncertainty
distribution, which together define the reward landscape for the
planner. The task is then to generate, for each vehicle, a tour, that
accumulates the highest possible uncertainty values. The tour is
traveled at a prescribed AUV speed while the duration is bounded above
by the assimilation period. the starting and ending point is selected
to simplify logistics for launch and recovery.

The tour itself is cast as a graph theoretic problem. The spatial
uncertainty map is first pre-processed to remove obstacles and
smoothened to highlight large-scale features. Candidate waypoints are
then identified from the map and used to build a weighted graph, where
nodes carry a reward proportional to their uncertainty value and edges
represent travel costs. The trajectory planning task is posed as a
tour routing problem where routes must balance the rewards obtained
from visiting nodes with associated costs of traveling between them
\cite{vidal2013,toth2014vehicle}. By solving this, the algorithm
returns a set of near-optimal trajectories that prioritize regions of
greatest uncertainty, while ensuring vehicle endurance and safety
constraints. This provides a principled way of steering autonomous
platforms toward the most informative sampling locations, forming a
key component in the daily cycle of forecast, adaptive sampling, and
data assimilation. Additional insights into the algorithm and its
deployment during \proj are reported elsewhere in
\cite{bernacchi2025}.

\kcomment{it MAY BE WORTH SAYING MORE ABOUT THESE PARAMETERS: DURATION
  OF THE TOUR, ASSIMILATION PERIOD, ETC. The section above is simply
  highlighting the method; its instantiation could come in the results
  section.}


\subsection{Operations and Data Collection}

\begin{figure}
    \centering
    \includegraphics[width=.7\linewidth]{fig/lauvs.png}
    \caption{AUVs deployed in the \proj experiment.}
    \label{fig:lauvs}
\end{figure}

In-situ data collection process involved three upper water-column
Light Autonomous Underwater Vehicles (LAUV) (XP2, XP3 and
XP5)\footnote{We will refer to these specific vehicles across the
  three day period.} which performed targeted sampling missions with
durations of up to 60 hours over the \naz Canyon region, with all
vehicles sampling the upper 100 m of the water column. These vehicles
designed and developed in-house at the University of Porto
\cite{sousa2012lauv} came equipped with a range of sensors including a
CTD (Fig. \ref{fig:lauvs}) along with WiFi and Iridium satellite
communications; the CTDs mounted on the AUVs were cross-calibrated to
ensure consistency. In addition to the hardware involved, an extensive
suite of mature mission planning and command/control tools were used
-- discussion of these is outside the scope of the paper and can be
referenced at
\cite{dias2005neptus,seacons10,toolchain2012,pinto2013lsts,Ferreira2018}.

The experiment had a small human and support vessel footprint,
demonstrating significant cost reduction in comparison to traditional
ship-based methods while measuring at higher spatial and temporal
resolution. Operations were conducted continuously by pairs of
operators working 6 hour shifts from command and control centers based
in Porto as well as in \naze. The intervention of small boats was
limited to the launch and recovery of AUVs, that operated unattended.


Challenging meteorological and ocean conditions constrained the number
of consecutive iterations of the
\emph{sample-assimilate-predict-direct} loop to a three-day sequence
of experiments (29\textsuperscript{th}-–31\textsuperscript{st}
October, 2025) (Fig. \ref{fig:scheme}). For the sake of clarity, the
presentation of the workflow and assimilation loop is focused only on
temperature measurements. As a consequence this work is univariate in
showing the impact of assimilated model-driven exploration. We believe
that results in Sec \ref{sec:results} are general enough to apply to
other critical oceanographic variables which would be modeled
similarly. The analysis in this work ultimately aims to quantify the
impact of assimilating AUV-acquired temperature data on the predictive
performance of the statistical model and to assess the operational
feasibility of the data cycle approach in a real-world coastal
setting.

Each experimental day followed a structured cycle involving: (i) the
generation of a statistical model forecast and its associated
uncertainty field based on the Copernicus Marine Service (CMS) data
available up to the previous day \cite{sotillo2021}; (ii) the
execution of the target sampling algorithm to plan the next day
missions; and (iii) in situ data collection by AUVs following the
algorithmically determined tours. The data collected within the
previous day were subsequently assimilated into the statistical model
to produce updated forecasts, enabling comparisons between forecasts
with and without data assimilation.


% \kcomment{WHAT IS TARGET SAMPLING IN THE FIGURE? THIS IS NOT SAMPLING,
%   THIS IS EXECUTING THE TOUR GENERATION ALGORITHM, RIGHT?  LIKEWISE
%   WHAT IS AUV DATA ACQUISITION? DATA AQUISITION HAS BEEN PERFORMED
%   DURING SAMPLING. THIS MAY BE DATA INGESTION AND QC -- send me the
%   source of the figure and I can fix it --KR} \rmcomment{i'll need to
%   redo the figure since I don't know where I put the ppt file... but,
%   yes, it is route/tour generation}

\begin{figure}
  \centering
  \includegraphics[scale=0.08]{fig/scheme.png}
  \caption{Three-day sequence
    (29\textsuperscript{th}–-31\textsuperscript{st} October 2025) of
    the sample–assimilate–predict–direct loop in the \proj
    experiment. Each cycle integrates CMS-based statistic model
    prediction, target sampling algorithm, AUV temperature data
    acQuisition and subsequent assimilation for the next day
    prediction}
  \label{fig:scheme}
\end{figure}

\subsubsection{A Multi-scenario framework}

Given challenges related to weather, assimilation and model
prediction, while operating multiple AUVs over multiple days, multiple
scenario solutions labeled \textbf{'A'} through \textbf{'D'}
contribute to the final solution set. These largely reflect the
impacts of operating in the harsh domain of the coastal environment.

On 29\textsuperscript{th} October, the statistical model produced the
initial forecast solution (A), which was used to plan the mission
executed by XP2 (Fig. \ref{fig:scheme}). The resulting temperature
data were later assimilated offline to produce an updated statistical
model solution (\textbf{B1}) for 30\textsuperscript{th}
October. Although operational real-time assimilation was initially
planned, logistical constraints prevented its implementation;
consequently, all assimilation was performed offline after mission
completion. The targeted sampling algorithm, therefore, relied on
statistical forecasts \emph{without} assimilation (\textbf{B}), using
pre-existing data as input for daily mission planning. This limitation
is not expected to have significantly affected the experimental
outcomes, as the operational area was spatially compact and the
predicted variability field remained consistent between consecutive
days. In future implementations, on-board or near-real-time
assimilation would need to be considered to fully exploit the adaptive
potential of the framework.

Data collected by XP3 and XP5 on 30\textsuperscript{th} October were
assimilated offline to generate new model solutions for
31\textsuperscript{st} October (\textbf{C1--C4}), along with a
non-assimilated reference case (\textbf{C}). The configurations were
as follows:

\begin{description}
\item[C1] – assimilation including only XP2 data from
  29\textsuperscript{th} October
\item[C2] – assimilation including XP2 (29\textsuperscript{th} October)
  and XP5 (30\textsuperscript{th} October) data
\item[C3] – assimilation including only XP5 data from
  30\textsuperscript{th} October
\item[C4] – assimilation including all available data from
  29\textsuperscript{th} and 30\textsuperscript{th} October (XP2, XP3,
  and XP5)
\end{description}

An analogous setup was used for case \textbf{D}, in which the
difference from case \textbf{C} is that the statistical predictions
for 31\textsuperscript{st} October were generated using CMS data
available as a prior up to different cutoff dates:

\begin{description}
\item[D] – forecast for 31\textsuperscript{st} October based on CMS
  data available as a prior until 29\textsuperscript{th} October
\item[D1–-D4] – corresponding to assimilation configurations
  \textbf{C1-–C4}, but using CMS data available prior until
  30\textsuperscript{th} October
\end{description}

Model performance was evaluated by comparing predicted and observed
temperature data along the AUV trajectories using the Root Mean Square
Error (RMSE) as the primary performance metric for
31\textsuperscript{st} October.

% \begin{figure}
%     \centering
%     %\includegraphics[width=.7\linewidth]{fig/temperatureprofiles.png}
%     \caption{INSERT schematic about the 3-day loop}
%     \label{fig:temperatureprofiles}
% \end{figure}

% PLEASE CHECK CAPTION OF FIGURE 3 - I have done this sequentially, except for this mod
%   \label{fig:loop-closure} 
